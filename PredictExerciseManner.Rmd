---
title: "Practical Machine Learning Course Project"
author: "VXG"
date: "4/24/2021"
output: 
    bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE)

```


```{r pkg}
#load packages 
library(caret)
library(dplyr)
library(ggplot2)
library(rapportools)
library(VIM)
library(rattle)

```

## Reading Data

The training data and test data are pulled from the Weight Lifting Exercises dataset from the publication cited below.

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

We will use this dataset to *predict how well the weight lifting exercise is performed* by the wearer based on activity data collected from the sensors placed on the *arm, forearm, belt, and dumbbell* of six healthy participants. The quality of exercise was classified into *five classes*:

- Class A: exercising exactly according to the specification
- Class B: throwing the elbows to the front
- Class C: lifting the dumbbell only halfway
- Class D: lowering the dumbbell only halfway
- Class E: throwing the hips to the front

```{r reading data}

#check if train and validation exists, if not load it. 
if (!exists("train", inherits = FALSE)) {
    train <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
}

if (!exists("test", inherits = FALSE)) {
    test <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
}

```

## Cleaning Data

We first examine how much data are missing from the datasets. The left plots show that large number of columns are missing more than 80% of data. The bottom half of the right plot shows that the missingness all occurs in the same rows, whereas the top half shows that only very few number of rows have no missing data.  

```{r plotmissing}

aggr(train)


```


### Exclude columns that are largely NAs or empty 

Since the Caret algorithms require complete observations without NAs, we need to remove the variables that are largely empty.  

```{r removeNA}

#count how many NAs or empty values there are in each column. 
missingness_byCol <- colSums(is.na(train) | train == "") #this returns a numeric vector of 160 numbers, each represent a missingness count for corresponding column. 

#get indices for the columns in which 90% of rows miss data. 
emptyCols <- which(missingness_byCol > dim(train)[1] * 0.9)  #this returns a numeric vector with indices for the largely empty columns. 

#remove these empty columns
train_s <- train[, - emptyCols]
test_s <- test[, - emptyCols]

```

### Exclude columns that lack variability

Some variables only have one or very few unique values relative to the number of samples. These variables won't be helpful for predicting the outcome and therefore should not be included in the model either. 

```{r removeLowVar}
#find the indices of columns that are near zero variability
nzv <- nearZeroVar(train_s)

#remove those columns
train_s <- train_s[, -nzv]
test_s <- test_s[, -nzv]

```


### Remove the first ID column to avoid interference with the algorithms

```{r removeID}

train_s <- train_s[, -1]
test_s <- test_s[, -1]

```

## Predictive Modeling

### Cross Validation
We will set aside the test dataset, and use only the train dataset in this section.
First we will partition the train dataset into a training set and a validation set for cross validation. 

```{r cv}

# Create a validation set - use 70% of the original training data for training, the remaining 30% for validation
set.seed(32323)
train_index <- createDataPartition(train_s$classe, p = 0.7, list = FALSE)
myTraining <- train_s[train_index, ]
myValidadtion <- train_s[-train_index, ] 

```

We will also specify the method as cross validation and number of folds as 3 using trainControl().

```{r fitcontrol}

fitControl <- trainControl(method = 'cv', number = 3)

```

We will attempt several different ML algorithms to predict the class:
- Decision Trees
- Random Forest
- Generalized Boosted Model


### Decision Trees 

The accuracy of the decision trees model is about 0.628, which translates to an out-of-sample error of 0.372. 

```{r decisiontrees}

mod_dt <- train(classe ~ ., data = myTraining, method = "rpart", trControl = fitControl)

# Plot
# fancyRpartPlot(mod_dt$finalModel)

# validate the model using validation dataset to evaluate the prediction accuracy
pred_dt <- predict(mod_dt, newdata = myValidadtion)
confusionMatrix(pred_dt, myValidadtion$classe)

```

### Random Forest

The accuracy of the random forest model is about 0.628, which translates to an out-of-sample error of 0.372. 

```{r rf}

mod_rf <- train(classe ~ ., data = myTraining, method = "rf", trControl = fitControl)

# validate the model using validation dataset to evaluate the prediction accuracy
pred_rf <- predict(mod_rf, newdata = myValidadtion)
confusionMatrix(pred_rf, myValidadtion$classe)

```


### Generalized Boosted Model

The accuracy of the generalized boosted model is about 0.628, which translates to an out-of-sample error of 0.372. 

```{r gbm}

mod_gbm <- train(classe ~ ., data = myTraining, method = "gbm", trControl = fitControl,  verbose=FALSE)

# validate the model using validation dataset to evaluate the prediction accuracy
pred_gbm <- predict(mod_gbm, newdata = myValidadtion)
confusionMatrix(pred_gbm, myValidadtion$classe)

```


## Prediction Results on Test set

Among the three ML algorithms attempted, GBM had the highest accuracy and therefore was selected for predicting classe of the test set. 

```{r predicttest}

predictions_test <- predict(mod_gbm, newdata = test_s)

predictions_test

```
